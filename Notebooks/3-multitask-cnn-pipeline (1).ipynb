{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2021025,"sourceType":"datasetVersion","datasetId":1209633,"isSourceIdPinned":false}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## built a two-stage pipeline:\n\n## 1. Segmentation with U-Net → localize tumors on BUSI ultrasound images.\n\n## 2. Classification with ResNet50 → classify segmented tumor patches into Normal, Benign, Malignan","metadata":{}},{"cell_type":"markdown","source":"## Install any missing packages","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:07:05.309261Z","iopub.execute_input":"2025-08-21T11:07:05.309469Z","iopub.status.idle":"2025-08-21T11:07:05.542447Z","shell.execute_reply.started":"2025-08-21T11:07:05.309452Z","shell.execute_reply":"2025-08-21T11:07:05.541455Z"}},"outputs":[{"name":"stdout","text":"Thu Aug 21 11:07:05 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install segmentation-models-pytorch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:07:08.576756Z","iopub.execute_input":"2025-08-21T11:07:08.577563Z","iopub.status.idle":"2025-08-21T11:08:27.594985Z","shell.execute_reply.started":"2025-08-21T11:07:08.577519Z","shell.execute_reply":"2025-08-21T11:08:27.593926Z"}},"outputs":[{"name":"stdout","text":"Collecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.33.1)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation-models-pytorch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Import libraries:","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:08:31.953848Z","iopub.execute_input":"2025-08-21T11:08:31.954173Z","iopub.status.idle":"2025-08-21T11:08:42.199338Z","shell.execute_reply.started":"2025-08-21T11:08:31.954143Z","shell.execute_reply":"2025-08-21T11:08:42.198758Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Step 1: Load BUSI Dataset","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"aryashah2k/breast-ultrasound-images-dataset\")\n\nprint(\"Path to dataset files:\", path)\n\n#Check dataset \n\nimport os\n\nDATA_PATH = \"/kaggle/input/breast-ultrasound-images-dataset\"  # adjust if needed\n\n# Check files\nfor root, dirs, files in os.walk(DATA_PATH):\n    if files:\n        print(root, \"->\", len(files), \"files\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:11.572032Z","iopub.execute_input":"2025-08-21T11:09:11.572503Z","iopub.status.idle":"2025-08-21T11:09:14.899228Z","shell.execute_reply.started":"2025-08-21T11:09:11.572479Z","shell.execute_reply":"2025-08-21T11:09:14.898373Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/breast-ultrasound-images-dataset\n/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign -> 891 files\n/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/normal -> 266 files\n/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant -> 421 files\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"##  1: Collect Images and Masks","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nDATA_PATH = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/\"\n\n# Find all images and masks\nall_files = glob(os.path.join(DATA_PATH, \"**/*.png\"), recursive=True)\n\n# Separate images and masks\nimages = [f for f in all_files if \"mask\" not in f.lower()]\nmasks = [f for f in all_files if \"mask\" in f.lower() or \"gt\" in f.lower()]\n\nprint(f\"Total images: {len(images)}\")\nprint(f\"Total masks: {len(masks)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:17.725602Z","iopub.execute_input":"2025-08-21T11:09:17.726126Z","iopub.status.idle":"2025-08-21T11:09:18.505101Z","shell.execute_reply.started":"2025-08-21T11:09:17.726103Z","shell.execute_reply":"2025-08-21T11:09:18.504261Z"}},"outputs":[{"name":"stdout","text":"Total images: 780\nTotal masks: 1578\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Match Images to Masks","metadata":{}},{"cell_type":"code","source":"# Map each image to its mask based on filename\nimage_to_mask = {}\nfor img_path in images:\n    base = os.path.basename(img_path).split('.')[0]\n    mask_match = [m for m in masks if base in os.path.basename(m)]\n    if mask_match:\n        image_to_mask[img_path] = mask_match[0]\n    else:\n        image_to_mask[img_path] = None  # fallback if mask missing\n\nprint(f\"Images with masks: {sum([m is not None for m in image_to_mask.values()])}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:21.289449Z","iopub.execute_input":"2025-08-21T11:09:21.289725Z","iopub.status.idle":"2025-08-21T11:09:21.760982Z","shell.execute_reply.started":"2025-08-21T11:09:21.289703Z","shell.execute_reply":"2025-08-21T11:09:21.760236Z"}},"outputs":[{"name":"stdout","text":"Images with masks: 780\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Train/val split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Keep only images with masks for segmentation\nsegmentation_imgs = [img for img, m in image_to_mask.items() if m is not None]\nsegmentation_masks = [image_to_mask[img] for img in segmentation_imgs]\n\ntrain_imgs, val_imgs, train_masks, val_masks = train_test_split(\n    segmentation_imgs, segmentation_masks, test_size=0.2, random_state=42\n)\n\nprint(\"Train images:\", len(train_imgs), \"Validation images:\", len(val_imgs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:25.569931Z","iopub.execute_input":"2025-08-21T11:09:25.570505Z","iopub.status.idle":"2025-08-21T11:09:25.580836Z","shell.execute_reply.started":"2025-08-21T11:09:25.570481Z","shell.execute_reply":"2025-08-21T11:09:25.579932Z"}},"outputs":[{"name":"stdout","text":"Train images: 624 Validation images: 156\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## U-Net Dataset Class","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport cv2\nimport numpy as np\n\nclass UltrasoundDataset(Dataset):\n    def __init__(self, images, masks, img_size=(256, 256), transform=None):\n        self.images = images\n        self.masks = masks\n        self.img_size = img_size\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        # Load grayscale\n        img = cv2.imread(self.images[idx], cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(self.masks[idx], cv2.IMREAD_GRAYSCALE)\n\n        # Resize\n        img = cv2.resize(img, self.img_size)\n        mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n\n        # Normalize\n        img = img / 255.0\n        mask = mask.astype(np.float32) / 255.0\n\n        # Add channel dimension\n        img = np.expand_dims(img, axis=0)\n        mask = np.expand_dims(mask, axis=0)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## U-Net Model (PyTorch)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super(UNet, self).__init__()\n\n        def conv_block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_c, out_c, 3, padding=1),\n                nn.ReLU(inplace=True)\n            )\n\n        # Encoder\n        self.enc1 = conv_block(in_channels, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = conv_block(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = conv_block(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n\n        # Bottleneck\n        self.bottleneck = conv_block(256, 512)\n\n        # Decoder\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = conv_block(512, 256)\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = conv_block(256, 128)\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = conv_block(128, 64)\n\n        # Final output\n        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder path\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n\n        # Bottleneck\n        b = self.bottleneck(self.pool3(e3))\n\n        # Decoder path\n        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n\n        # Output: segmentation mask probability\n        return torch.sigmoid(self.final(d1))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare DataLoaders","metadata":{}},{"cell_type":"markdown","source":"### Resize All Images and Masks Properly","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nclass UltrasoundDataset(Dataset):\n    def __init__(self, images, masks, img_size=(256, 256)):\n        self.images = images\n        self.masks = masks\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        # Read grayscale image\n        img = cv2.imread(self.images[idx], cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(self.masks[idx], cv2.IMREAD_GRAYSCALE)\n\n        # Resize both image and mask to fixed size\n        img = cv2.resize(img, self.img_size)             # bilinear default\n        mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n\n        # Normalize\n        img = img / 255.0\n        mask = mask / 255.0\n        mask = mask.astype(np.float32)\n\n        # Add channel dimension (C,H,W)\n        img = np.expand_dims(img, axis=0)\n        mask = np.expand_dims(mask, axis=0)\n\n        return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Use the updated UltrasoundDataset with img_size parameter\ntrain_dataset = UltrasoundDataset(train_imgs, train_masks, img_size=(256, 256))\nval_dataset   = UltrasoundDataset(val_imgs, val_masks, img_size=(256, 256))\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=4)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train U-Net","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = UNet().to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nfor epoch in range(10):  # start with small epochs\n    model.train()\n    train_loss = 0\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: ResNet50 classification","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries & Setup","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 2: Prepare Tumor Dataset","metadata":{}},{"cell_type":"markdown","source":"##   Prepare Dataset for ResNet50\n### 1. Convert grayscale to 3 channels because ResNet50 expects 3-channel input.\n\n### 2. Resize to 224×224 for ResNet50.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\n\n# 1️⃣ Define Dataset\nclass TumorDataset(Dataset):\n    def __init__(self, img_paths, labels, transform=None):\n        self.img_paths = img_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # ensure 3 channels\n        img = cv2.resize(img, (224,224))\n\n        if self.transform:\n            img = self.transform(img)\n\n        # Extra check: if somehow 1 channel remains, repeat it\n        if img.shape[0] == 1:  # [C,H,W]\n            img = img.repeat(3,1,1)\n\n        label = self.labels[idx]\n        return img, label\n\n# 2️⃣ Collect image paths & labels\nbase_path = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT\"\nclasses = [\"benign\", \"malignant\", \"normal\"]\nimg_paths, labels = [], []\n\nfor i, cls in enumerate(classes):\n    cls_folder = os.path.join(base_path, cls)\n    for f in os.listdir(cls_folder):\n        if f.endswith(\".png\") or f.endswith(\".jpg\"):\n            img_paths.append(os.path.join(cls_folder, f))\n            labels.append(i)  # 0=benign, 1=malignant, 2=normal\n\n# 3️⃣ Train/Test Split\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    img_paths, labels, test_size=0.2, random_state=42, stratify=labels\n)\n\n# 4️⃣ Transform for ResNet50\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\n# 5️⃣ DataLoaders\ntrain_dataset = TumorDataset(train_paths, train_labels, transform=transform)\nval_dataset   = TumorDataset(val_paths, val_labels, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\nprint(f\"✅ Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3 — Load Pretrained ResNet50","metadata":{}},{"cell_type":"code","source":"from torchvision.models import resnet50, ResNet50_Weights\n\nweights = ResNet50_Weights.DEFAULT  # most up-to-date pretrained weights\nmodel = resnet50(weights=weights)\n\n# Replace the final layer for 3-class classification\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, 3)\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n# 2️ Load Pretrained ResNet50\n\nnum_classes = 3  # benign, malignant, normal\nmodel = models.resnet50(pretrained=True)\n\n# Replace the final fully connected layer\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, num_classes)\nmodel = model.to(device)\n\n\n# 3️ Loss & Optimizer\n\ncriterion = nn.CrossEntropyLoss()  # suitable for multi-class\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nprint(f\" ResNet50 ready for {num_classes}-class classification on device: {device}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n\n# Training Setup\n\nnum_epochs = 10\nbest_val_acc = 0.0\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\nfor epoch in range(num_epochs):\n    \n    # Training\n  \n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * imgs.size(0)\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    train_losses.append(epoch_loss)\n    train_accs.append(epoch_acc)\n\n    # Validation\n   \n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    all_labels, all_preds = [], []\n\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item() * imgs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            val_correct += (predicted == labels).sum().item()\n            val_total += labels.size(0)\n\n            # Store labels & predictions for confusion matrix\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n    val_epoch_loss = val_loss / val_total\n    val_epoch_acc = val_correct / val_total\n    val_losses.append(val_epoch_loss)\n    val_accs.append(val_epoch_acc)\n\n    print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Train Acc={epoch_acc:.4f} | \"\n          f\"Val Loss={val_epoch_loss:.4f}, Val Acc={val_epoch_acc:.4f}\")\n\n    # ------------------------\n    # Save Best Model\n    # ------------------------\n    if val_epoch_acc > best_val_acc:\n        best_val_acc = val_epoch_acc\n        torch.save(model.state_dict(), \"best_resnet50.pth\")\n        print(\"✅ Saved best model\")\n\nprint(f\"Training complete. Best validation accuracy: {best_val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,5))\n\n\n# Loss Curve\n\nplt.subplot(1,2,1)\nplt.plot(train_losses, label='Train Loss', marker='o')\nplt.plot(val_losses, label='Validation Loss', marker='o')\nplt.title(\"Training & Validation Loss (3-Class)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\n\n\n# Accuracy Curve\n\nplt.subplot(1,2,2)\nplt.plot(train_accs, label='Train Accuracy', marker='o')\nplt.plot(val_accs, label='Validation Accuracy', marker='o')\nplt.title(\"Training & Validation Accuracy (3-Class)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6 — Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# ------------------------\n# Confusion Matrix\n# ------------------------\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(cm, display_labels=[\"Benign\", \"Malignant\", \"Normal\"])\ndisp.plot(cmap=plt.cm.Blues, values_format='d')  # integer values\nplt.title(\"Confusion Matrix (3-Class)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Labels must match: 0 → Normal, 1 → Benign, 2 → Malignan","metadata":{}},{"cell_type":"markdown","source":"# 1. Segmentation Visualization","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# def visualize_segmentation(img_path, mask_pred):\n#     \"\"\"\n#     img_path: path to original image\n#     mask_pred: predicted mask (numpy array, 256x256)\n#     \"\"\"\n#     img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n#     img = cv2.resize(img, (256,256))\n    \n#     plt.figure(figsize=(8,4))\n    \n#     # Original image\n#     plt.subplot(1,2,1)\n#     plt.imshow(img, cmap='gray')\n#     plt.title(\"Original Image\")\n#     plt.axis('off')\n    \n#     # Image + predicted mask overlay\n#     plt.subplot(1,2,2)\n#     plt.imshow(img, cmap='gray')\n#     plt.imshow(mask_pred, cmap='Reds', alpha=0.5)  # mask overlay\n#     plt.title(\"Segmentation Overlay\")\n#     plt.axis('off')\n    \n#     plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Classification Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport numpy as np\n\n# ------------------------\n# 1️⃣ Visualization Function\n# ------------------------\ndef visualize_classification(images, labels_true, labels_pred, class_names):\n    \"\"\"\n    images: list of tumor patches (224x224x3 or tensors)\n    labels_true: true class indices\n    labels_pred: predicted class indices\n    class_names: list of class names, e.g., ['Normal','Benign','Malignant']\n    \"\"\"\n    plt.figure(figsize=(12,6))\n    \n    for i in range(min(9, len(images))):\n        plt.subplot(3,3,i+1)\n        img = images[i]\n        \n        # Convert tensor to numpy if needed\n        if torch.is_tensor(img):\n            img = img.permute(1,2,0).cpu().numpy()  # C,H,W -> H,W,C\n            img = img * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406])  # unnormalize\n            img = np.clip(img, 0, 1)\n        \n        plt.imshow(img)\n        plt.title(f\"True: {class_names[labels_true[i]]}\\nPred: {class_names[labels_pred[i]]}\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# ------------------------\n# 2️⃣ Prepare validation labels\n# ------------------------\nval_labels = []\nfor path in val_paths:  # val_paths from your DataLoader split\n    if \"benign\" in path.lower():\n        val_labels.append(1)\n    elif \"malignant\" in path.lower():\n        val_labels.append(2)\n    else:  # normal\n        val_labels.append(0)\n\n# ------------------------\n# 3️⃣ Run model on validation set\n# ------------------------\n# Ensure model is defined and on correct device\nresnet = model.to(device)\nresnet.eval()\n\nlabels_pred = []\nwith torch.no_grad():\n    for imgs, _ in val_loader:\n        imgs = imgs.to(device)\n        outputs = resnet(imgs)\n        preds = torch.argmax(outputs, dim=1)\n        labels_pred.extend(preds.cpu().numpy())\n\n# ------------------------\n# 4️⃣ Extract images from dataset (unnormalized for plotting)\n# ------------------------\nval_imgs_for_plot = []\nfor img, _ in val_loader.dataset:\n    val_imgs_for_plot.append(img)\n\n# ------------------------\n# 5️⃣ Visualize predictions\n# ------------------------\nclass_names = ['Normal','Benign','Malignant']\nvisualize_classification(val_imgs_for_plot, val_labels, labels_pred, class_names)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T11:06:32.535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A single model to do both segmentation and classification simultaneously, that’s called a ** multi-task learning setup **\n## Multi-task Model Concept\n\n## Shared Encoder: Extracts features from the input image.\n\n## Segmentation Head: Outputs a pixel-wise mask (e.g., U-Net decoder).\n\n## Classification Head: Outputs a class label (e.g., via global pooling + fully connected layer).","metadata":{}},{"cell_type":"markdown","source":"### Benefits\n\nJointly learns features useful for both segmentation and classification.\n\nMay improve performance on small datasets.\n\nAvoids cropping & separate pipelines.","metadata":{}},{"cell_type":"markdown","source":"### Challenges\n\nHarder to train — balancing the two losses can be tricky.\n\nRequires images with both segmentation masks and class labels.\n\nDecoder size may need tuning depending on input resolution.","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Prepare Multi-Task Dataset\n\nSince each image has a corresponding mask and a class label, we need a Dataset class that returns:\n\nInput image (grayscale → RGB for ResNet encoder).\n\nSegmentation mask (binary mask).\n\nClass label (Normal=0, Benign=1, Malignant=2).","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\n# ------------------------\n# Multi-task Dataset\n# ------------------------\nclass BUSIMultiTaskDataset(Dataset):\n    def __init__(self, img_paths, mask_paths, labels, transform=None):\n        \"\"\"\n        img_paths: list of image file paths\n        mask_paths: list of corresponding mask file paths\n        labels: list of class labels (0=Normal,1=Benign,2=Malignant)\n        transform: optional transform for input images\n        \"\"\"\n        self.img_paths = img_paths\n        self.mask_paths = mask_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        # Load image\n        img = cv2.imread(self.img_paths[idx], cv2.IMREAD_GRAYSCALE)\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        img = cv2.resize(img, (256,256))\n\n        # Load mask\n        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n        mask = cv2.resize(mask, (256,256))\n        mask = mask / 255.0  # normalize 0-1\n        mask = mask.astype('float32')\n        mask = torch.tensor(mask).unsqueeze(0)  # [1,H,W]\n\n        # Class label\n        label = self.labels[idx]\n\n        # Apply image transform if any\n        if self.transform:\n            img = self.transform(img)\n\n        return img, mask, label\n\n# ------------------------\n# Define transforms for ResNet\n# ------------------------\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:43.018026Z","iopub.execute_input":"2025-08-21T11:09:43.018765Z","iopub.status.idle":"2025-08-21T11:09:43.358725Z","shell.execute_reply.started":"2025-08-21T11:09:43.018739Z","shell.execute_reply":"2025-08-21T11:09:43.357901Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Let’s collect all image paths, mask paths, and labels from your dataset folders so we can feed them into our BUSIMultiTaskDataset","metadata":{}},{"cell_type":"code","source":"# ------------------------\n# Dataset paths\n# ------------------------\nbase_path = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT\"\nclasses = [\"benign\", \"malignant\", \"normal\"]  # class folders\n\nimg_paths = []\nmask_paths = []\nlabels = []\n\nfor i, cls in enumerate(classes):\n    cls_folder = os.path.join(base_path, cls)\n    for f in os.listdir(cls_folder):\n        if f.endswith(\".png\") or f.endswith(\".jpg\"):\n            img_paths.append(os.path.join(cls_folder, f))\n            \n            # corresponding mask\n            # assuming masks are in the same folder with '_mask' in filename or separate folder\n            mask_file = f.replace(\".png\",\"_mask.png\").replace(\".jpg\",\"_mask.png\")\n            mask_path = os.path.join(base_path, f\"{f[:-4]}_mask.png\")\n            \n            # if mask exists\n            if os.path.exists(mask_path):\n                mask_paths.append(mask_path)\n            else:\n                # fallback: create dummy mask if missing\n                mask_paths.append(os.path.join(base_path, cls, f))\n            \n            labels.append(i)  # 0=benign,1=malignant,2=normal\n\nprint(f\"✅ Total images: {len(img_paths)}, Masks: {len(mask_paths)}, Labels: {len(labels)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:48.392733Z","iopub.execute_input":"2025-08-21T11:09:48.393281Z","iopub.status.idle":"2025-08-21T11:09:49.605555Z","shell.execute_reply.started":"2025-08-21T11:09:48.393261Z","shell.execute_reply":"2025-08-21T11:09:49.604889Z"}},"outputs":[{"name":"stdout","text":"✅ Total images: 1578, Masks: 1578, Labels: 1578\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Train/Validation Split + DataLoaders","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# ------------------------\n# Train/Validation Split\n# ------------------------\ntrain_imgs, val_imgs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n    img_paths, mask_paths, labels,\n    test_size=0.2, random_state=42, stratify=labels\n)\n\nprint(f\" Train samples: {len(train_imgs)}, Validation samples: {len(val_imgs)}\")\n\n# ------------------------\n# Create Datasets\n# ------------------------\ntrain_dataset = BUSIMultiTaskDataset(train_imgs, train_masks, train_labels, transform=transform)\nval_dataset   = BUSIMultiTaskDataset(val_imgs, val_masks, val_labels, transform=transform)\n\n# ------------------------\n# Create DataLoaders\n# ------------------------\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\nprint(\" DataLoaders ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:53.904630Z","iopub.execute_input":"2025-08-21T11:09:53.904940Z","iopub.status.idle":"2025-08-21T11:09:53.917098Z","shell.execute_reply.started":"2025-08-21T11:09:53.904921Z","shell.execute_reply":"2025-08-21T11:09:53.916398Z"}},"outputs":[{"name":"stdout","text":" Train samples: 1262, Validation samples: 316\n DataLoaders ready\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## We'll use ResNet50 as the encoder and add:\n\nSegmentation decoder → outputs a pixel-wise mask\n\nClassification head → outputs 3-class labe","metadata":{}},{"cell_type":"markdown","source":"## Define Multi-Task Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass MultiTaskUNet(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        \n        # ------------------------\n        # Encoder: Pretrained ResNet50\n        # ------------------------\n        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n        self.encoder = nn.Sequential(\n            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n            resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n        )\n        \n        # ------------------------\n        # Segmentation decoder\n        # ------------------------\n        self.seg_decoder = nn.Sequential(\n            nn.ConvTranspose2d(2048, 512, kernel_size=2, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 1, kernel_size=1),   # binary mask\n            nn.Sigmoid()\n        )\n        \n        # ------------------------\n        # Classification head\n        # ------------------------\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1,1)),\n            nn.Flatten(),\n            nn.Linear(2048, num_classes)\n        )\n    \n    def forward(self, x):\n        features = self.encoder(x)\n        mask = self.seg_decoder(features)\n        class_logits = self.classifier(features)\n        return mask, class_logits\n\n# ------------------------\n# Initialize model\n# ------------------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = MultiTaskUNet(num_classes=3).to(device)\nprint(f\"✅ Multi-task model ready on {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:09:58.190182Z","iopub.execute_input":"2025-08-21T11:09:58.190468Z","iopub.status.idle":"2025-08-21T11:09:59.848867Z","shell.execute_reply.started":"2025-08-21T11:09:58.190449Z","shell.execute_reply":"2025-08-21T11:09:59.848166Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 168MB/s] \n","output_type":"stream"},{"name":"stdout","text":"✅ Multi-task model ready on cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Loss Functions and Optimizer","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# ------------------------\n# Losses\n# ------------------------\nseg_criterion = nn.BCELoss()           # for segmentation mask\ncls_criterion = nn.CrossEntropyLoss()  # for classification\n\n# ------------------------\n# Optimizer\n# ------------------------\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# ------------------------\n# Training settings\n# ------------------------\nnum_epochs = 10\nbest_val_acc = 0.0\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:10:07.859881Z","iopub.execute_input":"2025-08-21T11:10:07.860158Z","iopub.status.idle":"2025-08-21T11:10:07.865766Z","shell.execute_reply.started":"2025-08-21T11:10:07.860141Z","shell.execute_reply":"2025-08-21T11:10:07.865070Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Multi-Task Training Loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for imgs, masks, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        imgs, masks, labels = imgs.to(device), masks.to(device), labels.to(device)\n        optimizer.zero_grad()\n        \n        # Forward pass\n        mask_pred, class_logits = model(imgs)\n        \n        # ------------------------\n        # Upsample mask_pred to match ground truth\n        # ------------------------\n        mask_pred = F.interpolate(mask_pred, size=(256,256), mode='bilinear', align_corners=False)\n        \n        # Compute losses\n        loss_seg = seg_criterion(mask_pred, masks)\n        loss_cls = cls_criterion(class_logits, labels)\n        \n        # Total loss = weighted sum\n        loss = loss_seg + loss_cls\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * imgs.size(0)\n        _, predicted = torch.max(class_logits, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    # Epoch metrics\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    train_losses.append(epoch_loss)\n    train_accs.append(epoch_acc)\n\n    # ------------------------\n    # Validation\n    # ------------------------\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    all_labels, all_preds = [], []\n\n    with torch.no_grad():\n        for imgs, masks, labels in val_loader:\n            imgs, masks, labels = imgs.to(device), masks.to(device), labels.to(device)\n            mask_pred, class_logits = model(imgs)\n            \n            # Upsample validation mask_pred\n            mask_pred = F.interpolate(mask_pred, size=(256,256), mode='bilinear', align_corners=False)\n            \n            loss_seg = seg_criterion(mask_pred, masks)\n            loss_cls = cls_criterion(class_logits, labels)\n            loss_val = loss_seg + loss_cls\n\n            val_loss += loss_val.item() * imgs.size(0)\n            _, predicted = torch.max(class_logits, 1)\n            val_correct += (predicted == labels).sum().item()\n            val_total += labels.size(0)\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n    val_epoch_loss = val_loss / val_total\n    val_epoch_acc = val_correct / val_total\n    val_losses.append(val_epoch_loss)\n    val_accs.append(val_epoch_acc)\n\n    print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Acc={epoch_acc:.4f} | Val Loss={val_epoch_loss:.4f}, Acc={val_epoch_acc:.4f}\")\n\n    # Save best model\n    if val_epoch_acc > best_val_acc:\n        best_val_acc = val_epoch_acc\n        torch.save(model.state_dict(), \"best_multitask_model.pth\")\n        print(\"✅ Saved best multi-task model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:10:11.301843Z","iopub.execute_input":"2025-08-21T11:10:11.302126Z","iopub.status.idle":"2025-08-21T11:16:56.292404Z","shell.execute_reply.started":"2025-08-21T11:10:11.302108Z","shell.execute_reply":"2025-08-21T11:16:56.291476Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 79/79 [00:40<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=1.1296, Acc=0.7441 | Val Loss=0.8781, Acc=0.8449\n✅ Saved best multi-task model\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 79/79 [00:32<00:00,  2.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.6106, Acc=0.9097 | Val Loss=0.5824, Acc=0.8861\n✅ Saved best multi-task model\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 79/79 [00:34<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.4823, Acc=0.9477 | Val Loss=0.5822, Acc=0.9177\n✅ Saved best multi-task model\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 79/79 [00:33<00:00,  2.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.4411, Acc=0.9643 | Val Loss=0.5584, Acc=0.9082\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.3971, Acc=0.9739 | Val Loss=0.5579, Acc=0.9304\n✅ Saved best multi-task model\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.3790, Acc=0.9810 | Val Loss=0.6331, Acc=0.9082\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 79/79 [00:35<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.3591, Acc=0.9857 | Val Loss=0.5470, Acc=0.9146\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 79/79 [00:34<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.3394, Acc=0.9849 | Val Loss=0.5997, Acc=0.9019\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 79/79 [00:34<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.3052, Acc=0.9952 | Val Loss=0.5185, Acc=0.9241\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 79/79 [00:34<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.3145, Acc=0.9897 | Val Loss=0.6034, Acc=0.9272\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport cv2\n\n# ------------------------\n# 1️ Visualization Function\n# ------------------------\ndef visualize_multitask(img_path, mask_pred, label_true, label_pred, class_names):\n    \"\"\"\n    img_path: path to original image\n    mask_pred: predicted mask (tensor or numpy, 256x256)\n    label_true: ground truth class index\n    label_pred: predicted class index\n    class_names: ['Normal','Benign','Malignant']\n    \"\"\"\n    # Load original image\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    img = cv2.resize(img, (256,256))\n    \n    # Convert mask_pred to numpy if tensor\n    if torch.is_tensor(mask_pred):\n        mask_pred = mask_pred.squeeze().cpu().numpy()\n    \n    plt.figure(figsize=(10,4))\n    \n    # Original image\n    plt.subplot(1,2,1)\n    plt.imshow(img, cmap='gray')\n    plt.title(f\"True Class: {class_names[label_true]}\")\n    plt.axis('off')\n    \n    # Image + predicted mask overlay\n    plt.subplot(1,2,2)\n    plt.imshow(img, cmap='gray')\n    plt.imshow(mask_pred, cmap='Reds', alpha=0.5)\n    plt.title(f\"Pred Class: {class_names[label_pred]}\")\n    plt.axis('off')\n    \n    plt.show()\n\n# ------------------------\n# 2️⃣ Run model on validation set and visualize first N examples\n# ------------------------\nclass_names = ['Normal','Benign','Malignant']\nnum_examples = 5\n\nmodel.eval()\nwith torch.no_grad():\n    for i, (imgs, masks, labels) in enumerate(val_loader):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        labels = labels.to(device)\n        \n        mask_pred, class_logits = model(imgs)\n        # Upsample mask_pred\n        mask_pred = torch.nn.functional.interpolate(mask_pred, size=(256,256), mode='bilinear', align_corners=False)\n        \n        preds = torch.argmax(class_logits, dim=1)\n        \n        for j in range(imgs.size(0)):\n            if i*val_loader.batch_size + j >= num_examples:\n                break\n            img_path = val_paths[i*val_loader.batch_size + j]  # original image path\n            visualize_multitask(img_path, mask_pred[j], labels[j].item(), preds[j].item(), class_names)\n        \n        if i*val_loader.batch_size >= num_examples:\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:17:06.273578Z","iopub.execute_input":"2025-08-21T11:17:06.274296Z","iopub.status.idle":"2025-08-21T11:17:06.611559Z","shell.execute_reply.started":"2025-08-21T11:17:06.274267Z","shell.execute_reply":"2025-08-21T11:17:06.610496Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1610539278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# original image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mvisualize_multitask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'val_paths' is not defined"],"ename":"NameError","evalue":"name 'val_paths' is not defined","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"## Segmentation Evaluation (U-Net style)\n\nWe will calculate Dice Score and IoU (Intersection over Union) for the predicted masks.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\n\ndef dice_score(pred, target, smooth=1e-6):\n    pred = pred.view(-1)\n    target = target.view(-1)\n    intersection = (pred * target).sum()\n    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n\ndef iou_score(pred, target, smooth=1e-6):\n    pred = pred.view(-1)\n    target = target.view(-1)\n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\n# Evaluate on validation set\nmodel.eval()\ndice_list, iou_list = [], []\n\nwith torch.no_grad():\n    for imgs, masks, labels in val_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        mask_pred, _ = model(imgs)\n        mask_pred = F.interpolate(mask_pred, size=(256,256), mode='bilinear', align_corners=False)\n        mask_pred_bin = (mask_pred > 0.5).float()\n\n        for i in range(len(masks)):\n            # Convert to CPU float\n            dice_val = dice_score(mask_pred_bin[i], masks[i]).cpu().item()\n            iou_val = iou_score(mask_pred_bin[i], masks[i]).cpu().item()\n            dice_list.append(dice_val)\n            iou_list.append(iou_val)\n\nprint(f\"✅ Segmentation - Dice Score: {np.mean(dice_list):.4f}, IoU: {np.mean(iou_list):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:23:46.892433Z","iopub.execute_input":"2025-08-21T11:23:46.892729Z","iopub.status.idle":"2025-08-21T11:23:52.350561Z","shell.execute_reply.started":"2025-08-21T11:23:46.892709Z","shell.execute_reply":"2025-08-21T11:23:52.349884Z"}},"outputs":[{"name":"stdout","text":"✅ Segmentation - Dice Score: 0.6506, IoU: 0.5503\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nos.makedirs(\"results/masks\", exist_ok=True)\n\nfor i, (img, mask) in enumerate(zip(imgs.cpu().numpy(), masks.cpu().numpy())):\n    plt.figure()\n    plt.imshow(img.transpose(1,2,0))  # C,H,W → H,W,C\n    plt.imshow(mask[0], alpha=0.5, cmap='Reds')\n    plt.axis('off')\n    plt.savefig(f\"results/masks/sample_{i}.png\")\n    plt.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:23:58.589102Z","iopub.execute_input":"2025-08-21T11:23:58.589645Z","iopub.status.idle":"2025-08-21T11:23:59.721550Z","shell.execute_reply.started":"2025-08-21T11:23:58.589622Z","shell.execute_reply":"2025-08-21T11:23:59.720802Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Classification Evaluation (ResNet / Multi-task)\n\nFor classification, report Accuracy and Confusion Matrix:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\nall_labels, all_preds = [], []\n\nwith torch.no_grad():\n    for imgs, masks, labels in val_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        _, class_logits = model(imgs)\n        _, predicted = torch.max(class_logits, 1)\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(predicted.cpu().numpy())\n\n# Accuracy\naccuracy = np.mean(np.array(all_preds) == np.array(all_labels))\nprint(f\"✅ Classification Accuracy: {accuracy:.4f}\")\n\n# Confusion Matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"results/confusion_matrix.png\")\nplt.show()\n\n# Optional: Detailed report\nprint(classification_report(all_labels, all_preds, target_names=classes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:17:26.609104Z","iopub.execute_input":"2025-08-21T11:17:26.609446Z","iopub.status.idle":"2025-08-21T11:17:33.013854Z","shell.execute_reply.started":"2025-08-21T11:17:26.609424Z","shell.execute_reply":"2025-08-21T11:17:33.012970Z"}},"outputs":[{"name":"stdout","text":"✅ Classification Accuracy: 0.9272\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n      benign       0.95      0.94      0.95       179\n   malignant       0.91      0.87      0.89        84\n      normal       0.86      0.96      0.91        53\n\n    accuracy                           0.93       316\n   macro avg       0.91      0.93      0.92       316\nweighted avg       0.93      0.93      0.93       316\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport zipfile\n\n# ------------------------\n# Setup\n# ------------------------\nos.makedirs(\"results/masks\", exist_ok=True)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.eval()\n\n# Lists to store metrics\ndice_list, iou_list = [], []\nall_labels, all_preds = [], []\n\n# ------------------------\n# Metrics functions\n# ------------------------\ndef dice_score(pred, target, smooth=1e-6):\n    pred = pred.view(-1)\n    target = target.view(-1)\n    intersection = (pred * target).sum()\n    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n\ndef iou_score(pred, target, smooth=1e-6):\n    pred = pred.view(-1)\n    target = target.view(-1)\n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum() - intersection\n    return (intersection + smooth) / (union + smooth + 1e-6)\n\n# ------------------------\n# Loop over validation set\n# ------------------------\nwith torch.no_grad():\n    for idx, (imgs, masks, labels) in enumerate(val_loader):\n        imgs, masks, labels = imgs.to(device), masks.to(device), labels.to(device)\n        mask_pred, class_logits = model(imgs)\n        mask_pred = F.interpolate(mask_pred, size=(256,256), mode='bilinear', align_corners=False)\n        mask_pred_bin = (mask_pred > 0.5).float()\n        \n        # Segmentation metrics\n        for i in range(len(masks)):\n            dice_val = dice_score(mask_pred_bin[i], masks[i]).cpu().item()\n            iou_val = iou_score(mask_pred_bin[i], masks[i]).cpu().item()\n            dice_list.append(dice_val)\n            iou_list.append(iou_val)\n            \n            # Save overlay images\n            img_np = imgs[i].cpu().numpy().transpose(1,2,0)\n            mask_np = masks[i][0].cpu().numpy()\n            pred_np = mask_pred_bin[i][0].cpu().numpy()\n            \n            plt.figure()\n            plt.imshow(img_np)\n            plt.imshow(mask_np, alpha=0.5, cmap='Greens')\n            plt.imshow(pred_np, alpha=0.5, cmap='Reds')\n            plt.axis('off')\n            plt.title(f\"Label: {labels[i].item()}\")\n            plt.savefig(f\"results/masks/sample_{idx}_{i}.png\")\n            plt.close()\n        \n        # Classification metrics\n        _, predicted = torch.max(class_logits, 1)\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(predicted.cpu().numpy())\n\n# ------------------------\n# Compute overall metrics\n# ------------------------\ndice_avg = np.mean(dice_list)\niou_avg  = np.mean(iou_list)\naccuracy = np.mean(np.array(all_preds) == np.array(all_labels))\nprint(f\"✅ Multi-task Results: Dice={dice_avg:.4f}, IoU={iou_avg:.4f}, Accuracy={accuracy:.4f}\")\n\n# ------------------------\n# Confusion Matrix\n# ------------------------\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"results/confusion_matrix.png\")\nplt.close()\n\n# ------------------------\n# Save metrics to file\n# ------------------------\nwith open(\"results/metrics.txt\", \"w\") as f:\n    f.write(f\"Multi-task Model Results\\n\")\n    f.write(f\"Dice Score: {dice_avg:.4f}\\n\")\n    f.write(f\"IoU: {iou_avg:.4f}\\n\")\n    f.write(f\"Classification Accuracy: {accuracy:.4f}\\n\")\n    f.write(\"\\nClassification Report:\\n\")\n    f.write(classification_report(all_labels, all_preds, target_names=classes))\n\n# ------------------------\n# Zip results for download\n# ------------------------\nzipf = zipfile.ZipFile('results_multitask.zip','w', zipfile.ZIP_DEFLATED)\nfor root, dirs, files in os.walk(\"results/\"):\n    for file in files:\n        zipf.write(os.path.join(root, file))\nzipf.close()\n\nprint(\"✅ Results saved in 'results/' folder and zipped as 'results_multitask.zip'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T11:24:12.557437Z","iopub.execute_input":"2025-08-21T11:24:12.557737Z","iopub.status.idle":"2025-08-21T11:25:10.305373Z","shell.execute_reply.started":"2025-08-21T11:24:12.557717Z","shell.execute_reply":"2025-08-21T11:25:10.304734Z"}},"outputs":[{"name":"stdout","text":"✅ Multi-task Results: Dice=0.6506, IoU=0.5060, Accuracy=0.9272\n✅ Results saved in 'results/' folder and zipped as 'results_multitask.zip'\n","output_type":"stream"}],"execution_count":22}]}
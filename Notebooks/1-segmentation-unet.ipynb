{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install any missing packages","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation-models-pytorch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 1: Load BUSI Dataset","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"aryashah2k/breast-ultrasound-images-dataset\")\n\nprint(\"Path to dataset files:\", path)\n\n#Check dataset \n\nimport os\n\nDATA_PATH = \"/kaggle/input/breast-ultrasound-images-dataset\"  # adjust if needed\n\n# Check files\nfor root, dirs, files in os.walk(DATA_PATH):\n    if files:\n        print(root, \"->\", len(files), \"files\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  1: Collect Images and Masks","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nDATA_PATH = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/\"\n\n# Find all images and masks\nall_files = glob(os.path.join(DATA_PATH, \"**/*.png\"), recursive=True)\n\n# Separate images and masks\nimages = [f for f in all_files if \"mask\" not in f.lower()]\nmasks = [f for f in all_files if \"mask\" in f.lower() or \"gt\" in f.lower()]\n\nprint(f\"Total images: {len(images)}\")\nprint(f\"Total masks: {len(masks)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Match Images to Masks","metadata":{}},{"cell_type":"code","source":"# Map each image to its mask based on filename\nimage_to_mask = {}\nfor img_path in images:\n    base = os.path.basename(img_path).split('.')[0]\n    mask_match = [m for m in masks if base in os.path.basename(m)]\n    if mask_match:\n        image_to_mask[img_path] = mask_match[0]\n    else:\n        image_to_mask[img_path] = None  # fallback if mask missing\n\nprint(f\"Images with masks: {sum([m is not None for m in image_to_mask.values()])}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train/val split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Keep only images with masks for segmentation\nsegmentation_imgs = [img for img, m in image_to_mask.items() if m is not None]\nsegmentation_masks = [image_to_mask[img] for img in segmentation_imgs]\n\ntrain_imgs, val_imgs, train_masks, val_masks = train_test_split(\n    segmentation_imgs, segmentation_masks, test_size=0.2, random_state=42\n)\n\nprint(\"Train images:\", len(train_imgs), \"Validation images:\", len(val_imgs))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## U-Net Dataset Class","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport cv2\nimport numpy as np\n\nclass UltrasoundDataset(Dataset):\n    def __init__(self, images, masks, img_size=(256, 256), transform=None):\n        self.images = images\n        self.masks = masks\n        self.img_size = img_size\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        # Load grayscale\n        img = cv2.imread(self.images[idx], cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(self.masks[idx], cv2.IMREAD_GRAYSCALE)\n\n        # Resize\n        img = cv2.resize(img, self.img_size)\n        mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n\n        # Normalize\n        img = img / 255.0\n        mask = mask.astype(np.float32) / 255.0\n\n        # Add channel dimension\n        img = np.expand_dims(img, axis=0)\n        mask = np.expand_dims(mask, axis=0)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## U-Net Model (PyTorch)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super(UNet, self).__init__()\n\n        def conv_block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_c, out_c, 3, padding=1),\n                nn.ReLU(inplace=True)\n            )\n\n        # Encoder\n        self.enc1 = conv_block(in_channels, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = conv_block(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = conv_block(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n\n        # Bottleneck\n        self.bottleneck = conv_block(256, 512)\n\n        # Decoder\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = conv_block(512, 256)\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = conv_block(256, 128)\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = conv_block(128, 64)\n\n        # Final output\n        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder path\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n\n        # Bottleneck\n        b = self.bottleneck(self.pool3(e3))\n\n        # Decoder path\n        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n\n        # Output: segmentation mask probability\n        return torch.sigmoid(self.final(d1))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare DataLoaders","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nclass UltrasoundDataset(Dataset):\n    def __init__(self, images, masks, img_size=(256, 256)):\n        self.images = images\n        self.masks = masks\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        # Read grayscale image\n        img = cv2.imread(self.images[idx], cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(self.masks[idx], cv2.IMREAD_GRAYSCALE)\n\n        # Resize both image and mask to fixed size\n        img = cv2.resize(img, self.img_size)             # bilinear default\n        mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n\n        # Normalize\n        img = img / 255.0\n        mask = mask / 255.0\n        mask = mask.astype(np.float32)\n\n        # Add channel dimension (C,H,W)\n        img = np.expand_dims(img, axis=0)\n        mask = np.expand_dims(mask, axis=0)\n\n        return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Use the updated UltrasoundDataset with img_size parameter\ntrain_dataset = UltrasoundDataset(train_imgs, train_masks, img_size=(256, 256))\nval_dataset   = UltrasoundDataset(val_imgs, val_masks, img_size=(256, 256))\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train U-Net","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = UNet().to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nfor epoch in range(10):  # start with small epochs\n    model.train()\n    train_loss = 0\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}